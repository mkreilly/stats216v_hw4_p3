---
output:
  pdf_document: default
  html_notebook: default
---
## Problem 3


```{r}
library(e1071)
library(ISLR)
attach(OJ)
```


### a) 
```{r}
set.seed(2017)

train = sample(1:nrow(OJ), 535)
test = setdiff(1:nrow(OJ), train)
```


### b)
```{r}
svm.fit <- svm(Purchase~.,data=OJ[train,],kernel="linear",cost=1)
summary(svm.fit)
```
The SVM machine has 202 support vectors with 102 and 100 in the classes.


### c) What are the training and test error rates?
```{r}
yhat <- predict(svm.fit)

table(predict=yhat, truth=OJ$Purchase[train])
```
The training error rate is the count of errors made over the tootal count or (40+37)/(201+80+132+122) = 0.14

```{r}
table(predict=yhat, truth=OJ$Purchase[test])
```
The test error rate is the count of errors made over the count of cases or (132+122)/(201+80+132+122) = 0.47.


### d)
```{r}
tune.out <- tune(svm,Purchase~.,data=OJ[train,],kernel="linear",ranges=list(cost=c(0.01,0.05,0.1,0.5,0.7,0.75,0.85,0.95,1,5,7.5,8,10)))
summary(tune.out)
```
An optimal cost is 0.75.


### e)
```{r}
bestmod <- tune.out$best.model
summary(bestmod)
```

```{r}
yhat <- predict(bestmod)

table(predict=yhat, truth=OJ$Purchase[train])
```
The training error rate is the count of errors made over the total count or (40+36)/535 = 0.14

```{r}
table(predict=yhat, truth=OJ$Purchase[test])
```
The test error rate is the count of errors made over the tootal count or (133+122)/535 = 0.48


(f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma.
```{r}
svmrad.fit <- svm(Purchase~.,data=OJ[train,],kernel="radial")
summary(svmrad.fit)
```
The SVM machine has 260 support vectors with 127 and 133 in the classes.


```{r}
yhat <- predict(svmrad.fit)

table(predict=yhat, truth=OJ$Purchase[train])
```
The training error rate is the count of errors made over the total count or (49+24)/535 = 0.136

```{r}
table(predict=yhat, truth=OJ$Purchase[test])
```
The test error rate is the count of errors made over the total count or (135+103)/535 = 0.445

```{r}

tunerad.out <- tune(svm,Purchase~.,data=OJ[train,],kernel="radial",ranges=list(cost=c(0.01,0.05,0.1,0.5,1,5,10)))
summary(tunerad.out)
```
Best model is at a cost of 5.


```{r}
bestmodrad <- tunerad.out$best.model
summary(bestmodrad)
```
This SVM has 218 support vectors wiht 105 and 113 in each class.


```{r}
yhat <- predict(bestmodrad)

table(predict=yhat, truth=OJ$Purchase[train])
```
The training error rate is the count of errors made over the total count or (42+24)/535 = 0.123


```{r}
table(predict=yhat, truth=OJ$Purchase[test])
```
The test error rate is the count of errors made over the total count or (132+107)/535 = 0.447


### g)
```{r}
svmply.fit <- svm(Purchase~.,data=OJ[train,],kernel="polynomial",degree=2)
summary(svmply.fit)
```
The SVM now has 315 support vectors with 153 and 162 in the classes.


```{r}
yhat <- predict(svmply.fit)

table(predict=yhat, truth=OJ$Purchase[train])
```
The training error rate is the count of errors made over the total count or (80+20)/535 = 0.187

```{r}
table(predict=yhat, truth=OJ$Purchase[test])
```
The test error rate is the count of errors made over the total count or (152+85)/535 = 0.443


```{r}
tuneply.out <- tune(svm,Purchase~.,data=OJ[train,],kernel="polynomial",ranges=list(cost=c(0.01,0.05,0.1,0.5,1,5,10)))
summary(tuneply.out)
```
The best model is at a cost of 10.


```{r}
bestmodply <- tuneply.out$best.model
summary(bestmodply)
```
This SVM has 215 support vectors with 108 and 108 in the classes.


```{r}
yhat <- predict(bestmodply)

table(predict=yhat, truth=OJ$Purchase[train])
```
The training error rate is the count of errors made over the total count or (43+17)/535 = 0.112


```{r}
table(predict=yhat, truth=OJ$Purchase[test])
```
The test error rate is the count of errors made over the total count or (135+102)/535 = 0.443


### h)
The results are not as good. This model is simpler in that it is a linear model; only the predictors have been transformed. This is a similar approach to transforming the predictors for a linear regression model.

```{r}
svmexp.fit <- svm(Purchase~.+poly(PriceDiff,2)+poly(ListPriceDiff,2)+poly(DiscMM,2)+poly(LoyalCH,2)+poly(SalePriceMM,2)+poly(SalePriceCH,2)+poly(PctDiscMM,2)+poly(PctDiscCH,2),data=OJ[train,],kernel="linear")
summary(svmexp.fit)
```
This SVM has 200 support vectors with 97 and 103 in the classes.

```{r}
yhat <- predict(svmexp.fit)

table(predict=yhat, truth=OJ$Purchase[train])
```
The training error rate is the count of errors made over the total count or (43+34)/535 = 0.146

```{r}
table(predict=yhat, truth=OJ$Purchase[test])
```
The test error rate is the count of errors made over the total count or (132+117)/535 = 0.465

```{r}

tuneexp.out <- tune(svm,Purchase~.+poly(PriceDiff,2)+poly(ListPriceDiff,2)+poly(DiscMM,2)+poly(LoyalCH,2)+poly(SalePriceMM,2)+poly(SalePriceCH,2)+poly(PctDiscMM,2)+poly(PctDiscCH,2),data=OJ[train,],kernel="linear",ranges=list(cost=c(0.01,0.05,0.1,0.5,1,5,10)))
summary(tuneexp.out)
```

```{r}
bestmodexp <- tuneexp.out$best.model
summary(bestmodexp)
```
This optimal SVM has a cost of 10 and 195 support vectors with 97 and 98 in the 2 classes.


```{r}
yhat <- predict(bestmodexp)

table(predict=yhat, truth=OJ$Purchase[train])


```
The training error rate is the count of errors made over the total count or (42+36)/535 = 0.146


```{r}
table(predict=yhat, truth=OJ$Purchase[test])
```
The test error rate is the count of errors made over the total count or (130+117)/535 = 0.462


### i)
The polynomial model has the best fit for the test data (0.443). Its 215 support vectors indicate that it is also at the mid-range of complexity for the various SVMs assesed here.
